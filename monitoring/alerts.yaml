groups:
  - name: akosha_critical
    interval: 30s
    rules:
      # Alert 1: High ingestion backlog
      - alert: HighIngestionBacklog
        expr: akosha_ingestion_queue_size > 1000
        for: 5m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: "Ingestion backlog critically high"
          description: "Backlog: {{ $value }} uploads waiting (threshold: 1000)"
          runbook: "https://docs.akosha.io/runbooks/INGESTION_BACKLOG.md"
          action: "Scale up ingestion pods, check for upstream issues"

      # Alert 2: Hot store size critical
      - alert: HotStoreSizeCritical
        expr: akosha_hot_store_size_bytes > 100e9  # 100GB
        for: 10m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: "Hot store exceeds 100GB threshold"
          description: "Size: {{ $value | humanize } } (100GB threshold)"
          runbook: "Trigger aging service immediately"
          action: "Check if aging service is running, verify tier migration"

      # Alert 3: High P99 latency
      - alert: HighQueryLatency
        expr: histogram_quantile(0.99, akosha_query_duration_seconds) > 2.0
        for: 5m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: "P99 query latency exceeds 2 seconds"
          description: "P99: {{ $value }}s (SLA: 500ms)"
          runbook: "Check shard health, enable cache warming, consider Milvus migration"
          action: "Review query patterns, scale query pods, enable L1/L2 cache"

  - name: akosha_warning
    interval: 1m
    rules:
      # Alert 4: Hot store size warning
      - alert: HotStoreSizeWarning
        expr: akosha_hot_store_size_bytes > 50e9  # 50GB
        for: 15m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "Hot store approaching 50GB"
          description: "Size: {{ $value | humanize } } (50GB threshold)"
          action: "Monitor aging service, prepare for migration"

      # Alert 5: P99 latency degradation
      - alert: QueryLatencyDegradation
        expr: histogram_quantile(0.99, akosha_query_duration_seconds) > 1.0
        for: 5m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "P99 query latency >1s degraded"
          description: "P99: {{ $value }}s (warning threshold: 1s)"
          action: "Review recent queries, check cache hit rate"

      # Alert 6: Low success rate
      - alert: LowIngestionSuccessRate
        expr: |
          sum(rate(akosha_ingestion_requests_total{status='success'}[5m]))
          /
          sum(rate(akosha_ingestion_requests_total[5m])) < 0.95
        for: 5m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "Ingestion success rate below 95%"
          description: "Success rate: {{ $value | humanize }}% (threshold: 95%)"
          action: "Check error logs, verify system health"

      # Alert 7: Cache hit rate low
      - alert: LowCacheHitRate
        expr: |
          sum(rate(akosha_query_cache_hits_total{cache_level='l1'}[5m]))
          /
          sum(rate(akosha_query_cache_hits_total[5m])) < 0.3
        for: 10m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "L1 cache hit rate below 30%"
          description: "Hit rate: {{ $value | humanize }}% (threshold: 30%)"
          action: "Review cache configuration, check query patterns"

  - name: akosha_info
    interval: 5m
    rules:
      # Alert 8: Mahavishnu unreachable
      - alert: MahavishnuUnreachable
        expr: up{job="mahavishnu-mcp"} == 0
        for: 2m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "Mahavishnu MCP unreachable"
          description: "Akosha operating in fallback mode"
          runbook: "https://docs.akosha.io/runbooks/MAHAVISHNU_DOWN.md"
          action: "Verify Mahavishnu status, check fallback mode operation"

      # Alert 9: Migration completed
      - alert: MigrationCompleted
        expr: increase(akosha_migration_records_total{status='success'}[1h]) > 0
        labels:
          severity: info
          team: sre
        annotations:
          summary: "Tier migration completed successfully"
          description: "Records migrated in last hour"
          action: "Verify data integrity, update monitoring"

      # Alert 10: System healthy
      - alert: AkoshaSystemHealthy
        expr: up{job=~"akosha.*"} == up{job=~"akosha.*"}
        labels:
          severity: info
          team: sre
        annotations:
          summary: "All Akosha systems healthy"
          description: "All pods reporting UP"

# Default routing for alerts
receivers:
  - name: 'default-receiver'
    email_configs:
      - to: 'akosha-alerts@example.com'
        from: 'akosha@monitoring.example.com'
        smarthostname: 'monitoring.example.com'
        send_resolved: false

# Route critical alerts to PagerDuty
routes:
  - match:
      severity: critical
    receiver: default-receiver
    continue: true

  - match:
      alertname: MahavishnuUnreachable
    receiver: default-receiver
